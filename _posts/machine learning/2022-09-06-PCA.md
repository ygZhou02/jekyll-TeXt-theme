---
title: "PCA"
tags: "Machine_Learning"
layout: "article"
article_header:
  type: cover
  image:
    src: /linear_algebra_cover.jpg

---

{:toc}

# Main Purpose of PCA

Principle Component Analysis (PCA), aims to reduce the number of dimension of original data. Assume original data has n dimensions. Our aim is to reflect those data to k dimensions, which are called principle component. Since k is less than n, one of the key problems of PCA is how to reserve as much information from the original n-dimensional data as possible. 

To depict this process in algebra, we assume original data as matrix $$X$$ of $$m \times n$$ shape. A project matrix W, in an $$n \times k$$ shape, multiplies with matrix $$X$$, and we obtain a matrix with lower dimension $$Y$$, in a $$m\times k$$ shape.
$$
X\; W = Y
$$
How to decide matrix $$W$$? We should optimize variance of processed data $$Y$$, because variance encodes the identifiability of processed data $$Y$$. This is the standard when we choose eigenvectors.

# Steps

input: m samples, n dimensions $$X$$
$$
X=\left[ \begin{matrix} x_{11}&x_{12}&...&x_{1n}\\
x_{21}&x_{22}&...&x_{2n}\\
...&...&...&...
\\x_{m1}&x_{m2}&...&x_{mn}
\end{matrix} \right]_{m\times n}
$$
each row of $$X$$ is a sample, while each column of $$X$$ is a dimension.

## Step1

firstly calculate average number of each dimension, denote $$\bar{X}_i=\frac1m\sum_{j=1}^mX_{ji},(i=1,2,...,n)$$

then do a normalizing transform, dubbed decentralization:
$$
\hat{X}=\left[ \begin{matrix} x_{11}-\bar{X_1}&x_{12}-\bar{X_2}&...&x_{1n}-\bar{X_n}\\
x_{21}-\bar{X_1}&x_{22}-\bar{X_2}&...&x_{2n}-\bar{X_n}\\
...&...&...&...
\\x_{m1}-\bar{X_1}&x_{m2}-\bar{X_2}&...&x_{mn}-\bar{X_n}
\end{matrix} \right]_{m\times n}
$$

## Step2

it's time to choose a sery of linearly independent vectors. In order to make the result matrix $$Y$$ contains as much information as possible, all columns of matrix $$Y$$ should be linearly independent. How to ensure this? If any two vectors are linearly independent, their dot product is 0. and as the formula of covariance is below:
$$
\sigma (x,y)=E(x-E(x))E(y-E(y))
$$
so the inner product of decentralized matrix is right the covariance. What's more, if two variables (dimensions) has a 0 covariance, their dot product is definitely 0. 

As the reasons above, the key point is to make sure all values in covariance matrix of $$Y$$ should be 0, except for those diagonal values. 

Due to some statistical reasons (actually it's because the calculation of variance expectation), covariance matrix of $$Y$$ is below:
$$
C'=\frac1{m-1}Y^TY=\frac1{m-1}(XW)^T(XW)\\=\frac1{m-1}W^TX^TXW=W^TCW
$$
matrix $$C$$ is the covariance matrix of original data $$X$$, $$C=\frac1{m-1}X^TX$$.

## Step3

As mentioned above, matrix $$C'$$ is supposed to be a diagonal matrix. Guess what? $$C'=W^TCW$$? It happens to be the definition of eigen vectors! Each column of $$W$$ matches with an eigen vector of matrix $$C$$ (because $$C$$, as a covariance matrix, must be a symmetric square matrix, it can be dicomposed by eigen vector decomposition). So what we should do is simplified as a calculation process of eigen vectors of matrix $$C$$.
$$
Cv_k=\lambda_kv_k
$$
In this step, we should take a choice among those eigen vectors. Our goal is to reduce the dimension down to k, so we need to choose k eigen vectors. Our standard is the eigen value of each eigen vector. Because eigen value is actually the diagonal value of result matrix $$Y$$'s covariance, a higher eigen value implies a more complicated information contained in this dimension. We aim to make $$Y$$ contains more information, so we just choose the eigen vectors as eigen values in a descending order.

## Step4

finally we get k useful eigen vectors. Concatenate those vectors together, we get matrix $$W$$. According to formula $$X\;W=Y$$, we can calculate the result matrix $$Y$$, whose columns are linearly independent, and variance has been already maximized. 